{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差伝播法の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは「重み」と「バイアス」を訓練データに適応するように調整する必要がある。\n",
    "これが学習フェーズ。\n",
    "\n",
    "1. (ミニバッチ)訓練データからランダムに、一部のデータを選ぶ\n",
    "2. (勾配の算出) 重みパラメーターに関する損失関数の勾配を求める\n",
    "    (どういう重みを設定すればよいのか)\n",
    "3. (パラメータの更新) 重みパラメーターを勾配方向に微小量だけ更新する\n",
    "    (答えに近づくために、勾配から見てパラメーターを調整する)\n",
    "4. 繰り返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装\n",
    "\n",
    "MNISTデータを使ったニューラルネットワークのため、  \n",
    "2つのレイヤーを持つニューラルネットワークを作ります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'sample')))\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        \"\"\"\n",
    "        重みとバイアスを初期化します\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : \n",
    "            入力層のニューロン数\n",
    "        hidden_size : \n",
    "            隠れ層のニューロン数\n",
    "        output_size : \n",
    "            出力層のニューロン数\n",
    "        weight_init_std : \n",
    "            重み初期化時のガウス分布のスケール\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        params : \n",
    "            ニューラルネットワークの重みとバイアス\n",
    "        layers : \n",
    "            レイヤーとなる関数。\n",
    "            辞書として引くのではなく、計算順序が大事になる\n",
    "        lastLayer :\n",
    "            出力層\n",
    "        \"\"\"\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict() # 順序付き辞書\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        推論をします\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.array\n",
    "            入力データの画像\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x : numpy.array\n",
    "            計算結果\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        損失関数の値を求めます\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.array\n",
    "            入力データの画像\n",
    "        t :\n",
    "            正解ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        accuracy :\n",
    "            損失\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        認識制度を求めます\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.array\n",
    "            入力データの画像\n",
    "        t :\n",
    "            正解ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        accuracy :\n",
    "            認識制度\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        重みパラメータに対する勾配を、数値微分で求めます\n",
    "        (処理が重い)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.array\n",
    "            入力データの画像\n",
    "        t :\n",
    "            正解ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grads :\n",
    "            勾配\n",
    "        \"\"\"\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        重みパラメータに対する勾配を、誤差逆伝播法で求めます\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.array\n",
    "            入力データの画像\n",
    "        t :\n",
    "            正解ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grads :\n",
    "            勾配\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数値微分は、実装が簡単だがその計算は重い。一方誤差伝搬法は実装が複雑だが計算が軽い。\n",
    "\n",
    "というわけで、誤差伝搬法の計算が本当に正しいのかを検証するために、**勾配確認(gradient check)** を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.690990345148965e-10->0.000000\n",
      "b1:2.8160295458966487e-09->0.000000\n",
      "W2:6.563031654817776e-09->0.000000\n",
      "b2:1.3939825101727532e-07->0.000000\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    # print(key + \":\" + str(diff))\n",
    "    print(key + \":\" + str(diff) + \"->\" + f'{diff:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習させてみよう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13013333333333332 0.1259\n",
      "0.9023833333333333 0.9067\n",
      "0.9237166666666666 0.9251\n",
      "0.9335333333333333 0.9328\n",
      "0.9443666666666667 0.9428\n",
      "0.9490833333333333 0.9473\n",
      "0.9538166666666666 0.949\n",
      "0.9593833333333334 0.9544\n",
      "0.9618333333333333 0.9579\n",
      "0.9639166666666666 0.9592\n",
      "0.96775 0.9609\n",
      "0.9700833333333333 0.9604\n",
      "0.97255 0.9624\n",
      "0.97375 0.9649\n",
      "0.975 0.9656\n",
      "0.9765166666666667 0.9663\n",
      "0.9776166666666667 0.9682\n",
      "実験終了!\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n",
    "\n",
    "print(\"実験終了!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "\n",
    "* 誤差伝搬法により、数値微分よりも速く学習をすることができる\n",
    "    * ただし複雑\n",
    "* 計算グラフによって、一部の計算だけをピックアップし、計算過程を視覚的に把握することができる\n",
    "* 計算グラフの順伝播は通常の計算、逆伝播は微分(偏微分)を求められる\n",
    "* 計算グラフから、誤差伝播法の実装方法がわかる\n",
    "* 誤差伝搬法の実装が、数値微分の実装結果とほぼ変わらないのかを確認するのに、勾配を見る"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
